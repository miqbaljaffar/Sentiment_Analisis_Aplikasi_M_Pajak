# -*- coding: utf-8 -*-
"""Sentiment Analisis Aplikasi M-Pajak

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIEkWbMmyf63Zacmppccc9eq8nkYHyac

Nama : Mohammad Iqbal Jaffar

Email : iqbaljaffar1108@gmail.com

# REVIEW SENTIMENT PADA APLIKASI M-PAJAK/CORETAX

# **Data Preparation**
"""

# Instalasi pustaka yang diperlukan
!pip install google-play-scraper -q
!pip install Sastrawi -q
!pip install tensorflow -q

# Mengimpor pustaka yang diperlukan
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
import re
import string
from io import StringIO
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from wordcloud import WordCloud
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from google_play_scraper import reviews_all, Sort

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer # CountVectorizer juga bisa jadi alternatif
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

import csv
import requests

# Mengatur opsi Pandas
pd.options.mode.chained_assignment = None

# Mengatur seed untuk reproduktibilitas
seed = 42
np.random.seed(seed)
tf.random.set_seed(seed)
import os
os.environ['TF_DETERMINISTIC_OPS'] = '1'

# Mengunduh dataset yang diperlukan untuk NLTK
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

"""## **Scraping Dataset Mandiri**"""

# ID Aplikasi Target (pajak di Google Play Store)
APP_ID = 'id.go.pajak.djp'
CSV_FILENAME = "pajak_reviews_min10k.csv"
MIN_SAMPLES = 10000

try:
    # Coba muat data yang sudah ada
    data = pd.read_csv(CSV_FILENAME)
    if len(data) < MIN_SAMPLES:
        print(f"Data yang ada hanya {len(data)} sampel. Menjalankan scraping untuk mendapatkan lebih banyak...")
        raise FileNotFoundError # Paksa scraping jika data kurang
    print(f"Berhasil memuat {len(data)} data dari {CSV_FILENAME}.")
except FileNotFoundError:
    print(f"File {CSV_FILENAME} tidak ditemukan atau data kurang. Melakukan scraping data ulasan...")
    # Mengambil minimal 10.000 ulasan, atau lebih untuk cadangan setelah pembersihan
    scraped_reviews = reviews_all(
        APP_ID,
        lang='id',
        country='id',
        sort=Sort.NEWEST, # Atau Sort.MOST_RELEVANT
        count=MIN_SAMPLES + 2000, # Ambil lebih untuk antisipasi data kosong/duplikat
        filter_score_with=None
    )

    df_scraped = pd.DataFrame(np.array(scraped_reviews), columns=['reviews'])
    df_scraped = df_scraped.join(pd.DataFrame(df_scraped.pop('reviews').tolist()))

    print(f"Scraping selesai, {len(df_scraped)} ulasan berhasil diambil.")
    df_scraped.to_csv(CSV_FILENAME, index=False)
    print(f"Data ulasan disimpan ke {CSV_FILENAME}")
    data = df_scraped.copy()

# Pemilihan Kolom dan Pembersihan Awal
if 'content' in data.columns and 'score' in data.columns:
    data = data[['content', 'score']]
    data.rename(columns={'score': 'rating', 'content': 'comment'}, inplace=True)
else:
    # Jika nama kolom berbeda karena versi scraper atau sumber lain
    # Sesuaikan dengan nama kolom yang benar dari hasil scraping Anda
    # Misalnya, jika kolomnya 'text' dan 'star'
    # data = data[['text', 'star']]
    # data.rename(columns={'star': 'rating', 'text': 'comment'}, inplace=True)
    print("PERINGATAN: Kolom 'content' atau 'score' tidak ditemukan. Periksa nama kolom hasil scraping Anda.")
    # Hentikan eksekusi atau tangani error jika kolom penting tidak ada

data.dropna(subset=['comment', 'rating'], inplace=True)
data = data[data['comment'].apply(lambda x: isinstance(x, str) and x.strip() != "")] # Pastikan komentar adalah string & tidak kosong
data['rating'] = pd.to_numeric(data['rating'], errors='coerce') # Pastikan rating numerik
data.dropna(subset=['rating'], inplace=True) # Hapus jika rating tidak valid
data.drop_duplicates(subset=['comment'], keep='first', inplace=True)

print(f"Jumlah data setelah loading dan pembersihan awal: {len(data)}")
if len(data) < MIN_SAMPLES:
    print(f"PERINGATAN: Jumlah data unik ({len(data)}) kurang dari target minimal {MIN_SAMPLES} setelah pembersihan. Anda mungkin perlu scraping lebih banyak.")
data.head()

"""## **Tahapan Ekstraksi Fitur**

### **Preprocessing Text**
"""

# Kamus untuk memperbaiki kata slang (bisa terus ditambahkan)
slangwords = {
    "bug": "kesalahan", "dll": "dan lain lain", "ckp": "cukup", "tw": "tahu", "tau": "tahu",
    "aku": "saya", "ga": "tidak", "gk":"tidak", "nggak":"tidak", "gak":"tidak", "kalo": "kalau", "ngelag": "lag", "nge lag": "lag",
    "CUAPEKKK": "capek", "tak": "tidak", "memory": "memori", "trus": "terus", "trs":"terus",
    "patah patah": "lag", "ku": "saya", "error": "kesalahan", "ane":"saya",
    "full": "penuh", "&": "dan", "Adli": "adil", "tdk": "tidak", "Dpt": "dapat",
    "tollol": "bodoh", "smua": "semua", "Thank's": "terima kasih", "thanks":"terima kasih",
    "game nya": "gim ini", "game": "gim", "bg": "abang", "sdh": "sudah", "bgt": "banget",
    "udah": "sudah", "jg": "juga", "lg": "lagi", "hp": "ponsel", "aj": "saja", "ae":"saja",
    "kpn": "kapan", "kyk": "seperti", "org": "orang", "grafik nya": "grafiknya", "grafik":"grafis",
    "tp": "tapi", "update": "pembaruan", "mn": "mana", "gajelas": "tidak jelas", "knp": "kenapa",
    "dr": "dari", "emg": "memang", "parahhh": "parah", "gacha": "gaca", "byk":"banyak",
    "mantap":"bagus", "keren":"bagus", "good":"bagus", "best":"terbaik", "nice":"bagus",
    "jelek":"buruk", "payah":"buruk", "parah":"buruk", "goblok":"bodoh", "tolol":"bodoh"
}

factory = StemmerFactory()
stemmer = factory.create_stemmer()

stop_words_id = list(stopwords.words('indonesian'))
stop_words_en = list(stopwords.words('english'))
custom_stopwords_list = {
    'sih', 'nya', 'nih', 'kok', 'deh', 'dong', 'mah', 'kek', 'sih', 'banget', 'amat', 'nan',
    'iya', 'yaa', 'gak', 'ga', 'ya', 'gaa', 'loh', 'kah', 'woi', 'woy', 'yg', 'dg', 'rt', 'dgn',
    'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'krn', 'jd', 'jgn', 'sdh',
    'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'wkwk', 'wkwkwk', 'haha', 'sksksk', 'anjay',
    'gue', 'gw', 'elu', 'lu', 'sy', 'aq', 'gimana', 'gitu', 'gini', 'gt', 'gn', 'ayo', 'yuk'
}
all_stopwords = set(stop_words_id + stop_words_en).union(custom_stopwords_list)

def cleaning_text(text):
    text = str(text).lower() # Case folding
    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text) # Hapus mention
    text = re.sub(r'#[A-Za-z0-9_]+', ' ', text) # Hapus hashtag
    text = re.sub(r'RT[\s]+', ' ', text) # Hapus RT
    text = re.sub(r"http\S+|www\S+|https\S+", ' ', text, flags=re.MULTILINE) # Hapus URL
    text = re.sub(r'\d+', ' ', text) # Hapus angka
    text = text.translate(str.maketrans("","",string.punctuation)) # Hapus tanda baca
    text = re.sub(r'\s+', ' ', text).strip() # Hapus spasi berlebih
    # Hapus karakter non-alfabet (opsional, bisa menghapus emoji atau simbol penting)
    # text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

def fix_slang_and_tokenize(text):
    words = text.split()
    # Normalisasi slang
    fixed_words = [slangwords.get(word, word) for word in words]
    # Tokenisasi (sebenarnya sudah jadi list of words)
    return fixed_words

def filter_stopwords_and_shortwords(tokens):
    # Hapus stopwords dan kata yang terlalu pendek (kurang dari 2 atau 3 huruf)
    return [token for token in tokens if token not in all_stopwords and len(token) > 2]

def stem_tokens(tokens):
    return [stemmer.stem(token) for token in tokens]

def to_sentence(list_words):
    return ' '.join(list_words)

# Pipeline Preprocessing
print("Memulai preprocessing teks...")
data['text_cleaned'] = data['comment'].apply(cleaning_text)
data['text_tokenized'] = data['text_cleaned'].apply(fix_slang_and_tokenize) # Ini sudah menghasilkan token
data['text_filtered'] = data['text_tokenized'].apply(filter_stopwords_and_shortwords)
data['text_stemmed'] = data['text_filtered'].apply(stem_tokens)
data['text_final'] = data['text_stemmed'].apply(to_sentence)

# Hapus baris yang teks finalnya kosong setelah preprocessing
data = data[data['text_final'] != '']
data.reset_index(drop=True, inplace=True)
print(f"Jumlah data setelah preprocessing teks dan penghapusan teks kosong: {len(data)}")
data[['comment', 'text_final', 'rating']].head()

"""### **Pelabelan Data (3 Kelas)**"""

# Berdasarkan 'rating', kita akan membuat 3 kelas: positif, netral, negatif.
def sentiment_label_from_rating(rating):
    rating = int(rating) # Pastikan integer
    if rating in [1, 2]:
        return 'negatif'
    elif rating == 3:
        return 'netral'
    elif rating in [4, 5]:
        return 'positif'
    return 'tidak diketahui' # Seharusnya tidak terjadi jika data rating bersih

data['sentiment'] = data['rating'].apply(sentiment_label_from_rating)
data = data[data['sentiment'] != 'tidak diketahui'] # Hapus jika ada label tidak diketahui

print("\nDistribusi Sentimen:")
print(data['sentiment'].value_counts())

# Visualisasi Distribusi Sentimen
plt.figure(figsize=(7, 5))
sns.countplot(x='sentiment', data=data, order=['positif', 'netral', 'negatif'], palette={'positif':'#2ECC71', 'netral':'#F1C40F', 'negatif':'#E74C3C'})
plt.title('Distribusi Sentimen Ulasan Setelah Pelabelan')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah Ulasan')
plt.show()

# Cek apakah ada cukup data untuk setiap kelas
min_samples_per_class = 100 # Bisa disesuaikan
class_counts = data['sentiment'].value_counts()
if (class_counts < min_samples_per_class).any():
    print(f"\nPeringatan: Beberapa kelas memiliki kurang dari {min_samples_per_class} sampel. Ini bisa mempengaruhi pelatihan model.")
    print(class_counts)
    # Pertimbangkan oversampling/undersampling jika sangat tidak seimbang dan model underperform.

# DataFrame final untuk model
data_model = data[['text_final', 'sentiment']].copy()
data_model.reset_index(drop=True, inplace=True)

print(f"\nJumlah data siap untuk model: {len(data_model)}")
print(data_model.head())

# Hapus variabel yang tidak terpakai untuk membebaskan memori
del data

"""### **Eksplorasi Label dengan Word Cloud**"""

# Pastikan kolom 'text_final' dan 'sentiment' ada
if 'text_final' in data_model.columns and 'sentiment' in data_model.columns:
    sentiment_categories = data_model['sentiment'].unique()
    for sentiment_cat in sentiment_categories:
        subset_texts = " ".join(data_model[data_model['sentiment'] == sentiment_cat]['text_final'].tolist())
        if subset_texts.strip(): # Hanya generate wordcloud jika ada teks
            try:
                wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False, random_state=seed).generate(subset_texts)
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.title(f'Word Cloud - Sentimen {sentiment_cat.capitalize()}')
                plt.axis('off')
                plt.show()
            except ValueError as e:
                print(f"Tidak bisa membuat word cloud untuk sentimen {sentiment_cat}: {e}")
        else:
            print(f"Tidak ada teks untuk menghasilkan word cloud sentimen {sentiment_cat}.")
else:
    print("Kolom 'text_final' atau 'sentiment' tidak ditemukan dalam DataFrame.")

"""# **Modelling**"""

# Variabel global untuk menyimpan hasil dan model
results_summary = []
best_models_and_vectorizers = {} # Untuk menyimpan model terbaik dan vectorizer/tokenizer terkait

# Fungsi untuk melatih dan evaluasi model ML Klasik
def train_evaluate_classical_model(X_train, y_train, X_test, y_test, model_pipeline, model_name, skema_name, feature_method, data_split_ratio):
    print(f"\n--- Melatih Model: {model_name} untuk {skema_name} ---")
    model_pipeline.fit(X_train, y_train) # Pipeline sudah termasuk vectorizer (jika ada) dan model

    y_pred_train = model_pipeline.predict(X_train)
    y_pred_test = model_pipeline.predict(X_test)

    acc_train = accuracy_score(y_train, y_pred_train)
    acc_test = accuracy_score(y_test, y_pred_test)

    print(f"Akurasi Training {model_name}: {acc_train:.4f}")
    print(f"Akurasi Testing {model_name}: {acc_test:.4f}")
    print("Laporan Klasifikasi (Test Set):")
    # Pastikan semua label ada di y_test dan y_pred_test untuk laporan yang benar
    unique_labels = sorted(list(set(y_test) | set(y_pred_test)))
    print(classification_report(y_test, y_pred_test, labels=unique_labels, target_names=unique_labels, zero_division=0))

    results_summary.append({
        'skema': skema_name,
        'model': model_name,
        'fitur': feature_method,
        'pembagian_data': data_split_ratio,
        'akurasi_train': acc_train,
        'akurasi_test': acc_test
    })

    if acc_test >= 0.85: # Kriteria minimal
        # Jika menggunakan pipeline sklearn, model dan vectorizer ada di dalamnya
        best_models_and_vectorizers[f"{skema_name}_{model_name}"] = {
            'pipeline': model_pipeline, # Simpan seluruh pipeline
            'accuracy_test': acc_test,
            'feature_type': feature_method,
            'model_type': 'classical'
        }
    return acc_train, acc_test, model_pipeline

# Data yang akan digunakan
X_texts = data_model['text_final']
y_labels = data_model['sentiment']

# Label Encoding untuk y (jika model tertentu membutuhkannya atau untuk konsistensi)
# Beberapa model sklearn bisa menangani label string, tapi ada baiknya di-encode
label_encoder_global = LabelEncoder()
y_encoded = label_encoder_global.fit_transform(y_labels)
sentiment_mapping = dict(zip(label_encoder_global.classes_, label_encoder_global.transform(label_encoder_global.classes_)))
print("Pemetaan Sentimen ke Angka:", sentiment_mapping)

# --- SKEMA 1: TF-IDF + Logistic Regression (Pembagian Data 80/20) ---
print("\n\n" + "="*10 + " SKEMA 1: TF-IDF + Logistic Regression (80/20) " + "="*10)
SKEMA_NAME_1 = "Skema 1"
FEATURE_METHOD_1 = "TF-IDF"
DATA_SPLIT_1 = "80/20"

X_train_s1, X_test_s1, y_train_s1, y_test_s1 = train_test_split(
    X_texts, y_labels, test_size=0.2, random_state=seed, stratify=y_labels
)

# Definisikan TF-IDF Vectorizer
tfidf_vectorizer_s1 = TfidfVectorizer(max_features=7000, min_df=3, max_df=0.7, ngram_range=(1,2))

# Pipeline untuk Logistic Regression
# (GridSearchCV bisa dilakukan di dalam pipeline atau di atasnya)
logreg_model = LogisticRegression(random_state=seed, solver='liblinear', class_weight='balanced', max_iter=1000)
pipeline_logreg = GridSearchCV(
    estimator=logreg_model,
    param_grid={'C': [0.01, 0.1, 1, 10]}, # solver dan penalty disesuaikan dengan liblinear
    cv=5, # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1
)

# Gabungkan vectorizer dan model dalam satu pipeline (untuk kemudahan, meski GridSearchCV dilakukan pada modelnya)
# Cara yang lebih tepat adalah GridSearch pada Pipeline-nya
from sklearn.pipeline import Pipeline
pipe_lr = Pipeline([
    ('tfidf', tfidf_vectorizer_s1),
    ('gridsearch_lr', pipeline_logreg) # gridsearch_lr di sini adalah estimatornya, bukan pipeline
])

# Untuk sementara, kita fit TF-IDF dulu, lalu GridSearchCV pada model
X_train_tfidf_s1 = tfidf_vectorizer_s1.fit_transform(X_train_s1)
X_test_tfidf_s1 = tfidf_vectorizer_s1.transform(X_test_s1)

print(f"Bentuk matriks TF-IDF Training (Skema 1): {X_train_tfidf_s1.shape}")
pipeline_logreg.fit(X_train_tfidf_s1, y_train_s1)
print(f"Parameter terbaik Logistic Regression (Skema 1): {pipeline_logreg.best_params_}")
lr_best_s1 = pipeline_logreg.best_estimator_

# Untuk menyimpan ke best_models_and_vectorizers, kita perlu simpan vectorizer dan model terbaiknya
# Kita buat pipeline manual setelah mendapatkan model terbaik
final_pipeline_lr_s1 = Pipeline([
    ('tfidf', tfidf_vectorizer_s1), # Vectorizer yang sudah di-fit ke data training S1
    ('classifier', lr_best_s1)
])

# Evaluasi menggunakan data mentah (pipeline akan transform X_test_s1)
train_evaluate_classical_model(X_train_s1, y_train_s1, X_test_s1, y_test_s1, final_pipeline_lr_s1, "Logistic Regression (Best)", SKEMA_NAME_1, FEATURE_METHOD_1, DATA_SPLIT_1)

# --- SKEMA 2: TF-IDF + SVM (Pembagian Data 75/25 atau Algoritma Berbeda) ---
print("\n\n" + "="*10 + " SKEMA 2: TF-IDF + SVM (75/25) " + "="*10)
SKEMA_NAME_2 = "Skema 2"
FEATURE_METHOD_2 = "TF-IDF" # Masih menggunakan TF-IDF
DATA_SPLIT_2 = "75/25"

X_train_s2, X_test_s2, y_train_s2, y_test_s2 = train_test_split(
    X_texts, y_labels, test_size=0.25, random_state=seed, stratify=y_labels # Pembagian data berbeda
)

# Gunakan TF-IDF Vectorizer baru atau yang sudah ada (jika ingin konsisten fiturnya)
# Kita buat baru untuk skema ini agar independen, tapi bisa juga pakai yang dari Skema 1
tfidf_vectorizer_s2 = TfidfVectorizer(max_features=7000, min_df=3, max_df=0.7, ngram_range=(1,2))
X_train_tfidf_s2 = tfidf_vectorizer_s2.fit_transform(X_train_s2)
X_test_tfidf_s2 = tfidf_vectorizer_s2.transform(X_test_s2)
print(f"Bentuk matriks TF-IDF Training (Skema 2): {X_train_tfidf_s2.shape}")

# Model SVM dengan GridSearchCV
# SVM bisa lambat, jadi parameter grid mungkin perlu disederhanakan atau gunakan subset data untuk tuning
svm_model = SVC(random_state=seed, class_weight='balanced', probability=True) # probability=True untuk predict_proba
pipeline_svm = GridSearchCV(
    estimator=svm_model,
    param_grid={'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']},
    cv=3, # Kurangi CV fold untuk kecepatan jika perlu
    scoring='accuracy',
    n_jobs=-1
)

pipeline_svm.fit(X_train_tfidf_s2, y_train_s2)
print(f"Parameter terbaik SVM (Skema 2): {pipeline_svm.best_params_}")
svm_best_s2 = pipeline_svm.best_estimator_

final_pipeline_svm_s2 = Pipeline([
    ('tfidf', tfidf_vectorizer_s2),
    ('classifier', svm_best_s2)
])
train_evaluate_classical_model(X_train_s2, y_train_s2, X_test_s2, y_test_s2, final_pipeline_svm_s2, "SVM (Best)", SKEMA_NAME_2, FEATURE_METHOD_2, DATA_SPLIT_2)

# Alternatif untuk Skema 2: Random Forest
print("\n\n" + "="*10 + " SKEMA 2 (Alternatif): TF-IDF + Random Forest (75/25) " + "="*10)
SKEMA_NAME_2_ALT = "Skema 2 (Alt RF)"
rf_model = RandomForestClassifier(random_state=seed, class_weight='balanced', n_jobs=-1)
pipeline_rf = GridSearchCV(
    estimator=rf_model,
    param_grid={'n_estimators': [100, 200], 'max_depth': [None, 20, 30], 'min_samples_split': [2, 5]},
    cv=3,
    scoring='accuracy',
    n_jobs=-1
)

pipeline_rf.fit(X_train_tfidf_s2, y_train_s2) # Menggunakan X_train_tfidf_s2 yang sama
print(f"Parameter terbaik Random Forest (Skema 2 Alt): {pipeline_rf.best_params_}")
rf_best_s2_alt = pipeline_rf.best_estimator_

final_pipeline_rf_s2_alt = Pipeline([
    ('tfidf', tfidf_vectorizer_s2), # Gunakan vectorizer yang sama dengan SVM di Skema 2
    ('classifier', rf_best_s2_alt)
])
train_evaluate_classical_model(X_train_s2, y_train_s2, X_test_s2, y_test_s2, final_pipeline_rf_s2_alt, "Random Forest (Best)", SKEMA_NAME_2_ALT, FEATURE_METHOD_2, DATA_SPLIT_2)

# --- SKEMA 3: Deep Learning (Bidirectional LSTM) ---
print("\n\n" + "="*10 + " SKEMA 3: Deep Learning - BiLSTM (80/20) " + "="*10)
SKEMA_NAME_3 = "Skema 3 (Deep Learning)"
FEATURE_METHOD_3 = "Keras Embedding"
DATA_SPLIT_3 = "80/20 (dengan validation_split)"

# Persiapan Data untuk Deep Learning
MAX_NUM_WORDS = 15000  # Ukuran vocabulary (sesuaikan berdasarkan data Anda)
MAX_SEQUENCE_LENGTH = 120 # Panjang sekuens maksimum (sesuaikan)
EMBEDDING_DIM = 128 # Dimensi embedding vector

keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<oov>")
keras_tokenizer.fit_on_texts(X_texts) # Fit pada semua data teks
word_index = keras_tokenizer.word_index
print(f"Ditemukan {len(word_index)} token unik.")

X_sequences = keras_tokenizer.texts_to_sequences(X_texts)
X_padded_sequences = pad_sequences(X_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

# Label sudah di-encode menjadi y_encoded (numerik)
# Sekarang ubah ke one-hot encoding untuk categorical_crossentropy
y_one_hot = to_categorical(y_encoded, num_classes=len(label_encoder_global.classes_))

# Pembagian Data untuk Deep Learning
X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(
    X_padded_sequences, y_one_hot, test_size=0.2, random_state=seed, stratify=y_encoded # stratify dengan y sebelum one-hot
)
print(f"Bentuk X_train_dl: {X_train_dl.shape}, y_train_dl: {y_train_dl.shape}")
print(f"Bentuk X_test_dl: {X_test_dl.shape}, y_test_dl: {y_test_dl.shape}")

# Model Deep Learning: Bidirectional LSTM
def create_bilstm_model(num_classes, max_words, embedding_dim, max_seq_len):
    model = Sequential([
        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_len),
        Bidirectional(LSTM(64, return_sequences=True)),
        Dropout(0.4), # Meningkatkan dropout
        Bidirectional(LSTM(32)),
        Dropout(0.4), # Meningkatkan dropout
        Dense(64, activation='relu'),
        Dropout(0.4),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Sesuaikan learning rate jika perlu
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

bilstm_model = create_bilstm_model(
    num_classes=y_one_hot.shape[1],
    max_words=MAX_NUM_WORDS,
    embedding_dim=EMBEDDING_DIM,
    max_seq_len=MAX_SEQUENCE_LENGTH
)
bilstm_model.summary()

# Callbacks
early_stopping_dl = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1) # Tingkatkan patience
reduce_lr_dl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-6, verbose=1)

# Pelatihan Model LSTM
BATCH_SIZE = 32 # Kurangi batch size jika memori terbatas atau untuk regularisasi
EPOCHS = 30 # Tingkatkan epoch, early stopping akan mengontrol

print(f"\n--- Melatih Model: Bidirectional LSTM untuk {SKEMA_NAME_3} ---")
history_bilstm = bilstm_model.fit(
    X_train_dl, y_train_dl,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.15, # Ambil 15% dari X_train_dl untuk validasi selama training
    callbacks=[early_stopping_dl, reduce_lr_dl],
    verbose=1
)

# Evaluasi Model LSTM
loss_test_dl, acc_test_dl = bilstm_model.evaluate(X_test_dl, y_test_dl, verbose=0)

# Akurasi training dari epoch terbaik (jika early stopping) atau prediksi ulang
y_pred_train_dl_probs = bilstm_model.predict(X_train_dl)
y_pred_train_dl_labels = np.argmax(y_pred_train_dl_probs, axis=1)
y_train_dl_actual_labels = np.argmax(y_train_dl, axis=1)
acc_train_dl = accuracy_score(y_train_dl_actual_labels, y_pred_train_dl_labels)

print(f"Akurasi Training Bidirectional LSTM: {acc_train_dl:.4f}")
print(f"Akurasi Testing Bidirectional LSTM: {acc_test_dl:.4f}")

y_pred_test_dl_probs = bilstm_model.predict(X_test_dl)
y_pred_test_dl_labels = np.argmax(y_pred_test_dl_probs, axis=1)
y_test_dl_actual_labels = np.argmax(y_test_dl, axis=1)

print("Laporan Klasifikasi LSTM (Test Set):")
print(classification_report(y_test_dl_actual_labels, y_pred_test_dl_labels,
                            labels=label_encoder_global.transform(label_encoder_global.classes_), # Gunakan angka dari encoder
                            target_names=label_encoder_global.classes_, zero_division=0))

results_summary.append({
    'skema': SKEMA_NAME_3,
    'model': "Bidirectional LSTM",
    'fitur': FEATURE_METHOD_3,
    'pembagian_data': DATA_SPLIT_3,
    'akurasi_train': acc_train_dl,
    'akurasi_test': acc_test_dl
})

if acc_test_dl >= 0.85:
    best_models_and_vectorizers[f"{SKEMA_NAME_3}_BiLSTM"] = {
        'model_object': bilstm_model,
        'tokenizer_keras': keras_tokenizer,
        'max_len': MAX_SEQUENCE_LENGTH,
        'label_encoder': label_encoder_global, # Penting untuk inference
        'accuracy_test': acc_test_dl,
        'feature_type': FEATURE_METHOD_3,
        'model_type': 'deep_learning'
    }

# Plot Akurasi dan Loss untuk LSTM
if history_bilstm:
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history_bilstm.history['accuracy'], label='Train Accuracy')
    plt.plot(history_bilstm.history['val_accuracy'], label='Validation Accuracy')
    plt.title('BiLSTM Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(history_bilstm.history['loss'], label='Train Loss')
    plt.plot(history_bilstm.history['val_loss'], label='Validation Loss')
    plt.title('BiLSTM Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

"""## **Ringkasan Hasil dan Pemilihan Model Terbaik**"""

print("\n\n" + "="*20 + " RINGKASAN HASIL EKSPERIMEN " + "="*20)
df_results = pd.DataFrame(results_summary)
df_results = df_results.sort_values(by='akurasi_test', ascending=False)
print(df_results)

print("\n\n" + "="*20 + " MODEL TERBAIK YANG DISIMPAN (Akurasi Test >= 85%) " + "="*20)
if best_models_and_vectorizers:
    for model_id, details in best_models_and_vectorizers.items():
        print(f"ID Model: {model_id}, Akurasi Test: {details['accuracy_test']:.4f}, Tipe Fitur: {details['feature_type']}")
else:
    print("Tidak ada model yang mencapai akurasi test minimal 85%.")

# Pilih model terbaik secara keseluruhan untuk inference
best_overall_model_id = None
highest_overall_acc = 0.0
if best_models_and_vectorizers:
    # Prioritaskan model Deep Learning jika akurasinya >= 92%, atau model dengan akurasi tertinggi jika tidak
    dl_models_above_92 = {k: v for k, v in best_models_and_vectorizers.items() if v['model_type'] == 'deep_learning' and v['accuracy_test'] >= 0.92}
    if dl_models_above_92:
        best_overall_model_id = max(dl_models_above_92, key=lambda k: dl_models_above_92[k]['accuracy_test'])
        highest_overall_acc = dl_models_above_92[best_overall_model_id]['accuracy_test']
    else: # Jika tidak ada DL >= 92%, ambil yang tertinggi dari semua yang >= 85%
        best_overall_model_id = max(best_models_and_vectorizers, key=lambda k: best_models_and_vectorizers[k]['accuracy_test'])
        highest_overall_acc = best_models_and_vectorizers[best_overall_model_id]['accuracy_test']

    print(f"\nModel terbaik untuk inference: {best_overall_model_id} dengan akurasi test: {highest_overall_acc:.4f}")
else:
    print("\nTidak ada model yang memenuhi syarat untuk inference.")